{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesla Stock Analysis with PySpark\n",
    "\n",
    "This notebook demonstrates how to use Apache Spark to analyze Tesla (TSLA) stock data.\n",
    "We'll explore various Spark DataFrame operations, perform statistical analysis, and create visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Spark Session Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Data visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/09/10 09:03:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Spark Version: 3.3.1\n",
      "Spark Application ID: local-1757520205864\n",
      "Spark UI: http://iad50-br-lbe-j9-r3-xe-0-0-1-0.amazon.com:4040\n",
      "\n",
      "Spark session initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Tesla Stock Analysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 20) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(\"\\nSpark session initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Total records: 1,257\n",
      "Number of columns: 6\n"
     ]
    }
   ],
   "source": [
    "# Load the TSLA CSV file\n",
    "file_path = \"data/tsla.csv\"\n",
    "\n",
    "# Read CSV with schema inference\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(file_path)\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Total records: {df.count():,}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema:\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of the dataset:\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+\n",
      "|Date               |Open              |High              |Low               |Close             |Volume   |\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+\n",
      "|2020-01-02 00:00:00|28.299999237060547|28.713333129882812|28.11400032043457 |28.68400001525879 |142981500|\n",
      "|2020-01-03 00:00:00|29.366666793823242|30.266666412353516|29.128000259399414|29.534000396728516|266677500|\n",
      "|2020-01-06 00:00:00|29.364667892456055|30.104000091552734|29.333332061767578|30.1026668548584  |151995000|\n",
      "|2020-01-07 00:00:00|30.760000228881836|31.441999435424805|30.224000930786133|31.270666122436523|268231500|\n",
      "|2020-01-08 00:00:00|31.579999923706055|33.232666015625   |31.215333938598633|32.80933380126953 |467164500|\n",
      "|2020-01-09 00:00:00|33.13999938964844 |33.253334045410156|31.524667739868164|32.089332580566406|426606000|\n",
      "|2020-01-10 00:00:00|32.11933135986328 |32.3293342590332  |31.579999923706055|31.876667022705078|194392500|\n",
      "|2020-01-13 00:00:00|32.900001525878906|35.04199981689453 |32.79999923706055 |34.990665435791016|397764000|\n",
      "|2020-01-14 00:00:00|36.284000396728516|36.49399948120117 |34.99333190917969 |35.861331939697266|434943000|\n",
      "|2020-01-15 00:00:00|35.31733322143555 |35.85599899291992 |34.452667236328125|34.56666564941406 |260532000|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first 10 rows\n",
    "print(\"First 10 rows of the dataset:\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date column converted and date components extracted.\n",
      "+----------+----+-----+-------+---+---------+\n",
      "|      Date|Year|Month|Quarter|Day|DayOfWeek|\n",
      "+----------+----+-----+-------+---+---------+\n",
      "|2020-01-02|2020|    1|      1|  2|        5|\n",
      "|2020-01-03|2020|    1|      1|  3|        6|\n",
      "|2020-01-06|2020|    1|      1|  6|        2|\n",
      "|2020-01-07|2020|    1|      1|  7|        3|\n",
      "|2020-01-08|2020|    1|      1|  8|        4|\n",
      "+----------+----+-----+-------+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Date column to proper date type and extract date components\n",
    "df = df.withColumn(\"Date\", F.to_date(F.col(\"Date\"))) \\\n",
    "       .withColumn(\"Year\", F.year(\"Date\")) \\\n",
    "       .withColumn(\"Month\", F.month(\"Date\")) \\\n",
    "       .withColumn(\"Day\", F.dayofmonth(\"Date\")) \\\n",
    "       .withColumn(\"DayOfWeek\", F.dayofweek(\"Date\")) \\\n",
    "       .withColumn(\"Quarter\", F.quarter(\"Date\"))\n",
    "\n",
    "# Cache the DataFrame for better performance\n",
    "df.cache()\n",
    "\n",
    "print(\"Date column converted and date components extracted.\")\n",
    "df.select(\"Date\", \"Year\", \"Month\", \"Quarter\", \"Day\", \"DayOfWeek\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in each column:\n",
      "+----+----+----+---+-----+------+----+-----+---+---------+-------+\n",
      "|Date|Open|High|Low|Close|Volume|Year|Month|Day|DayOfWeek|Quarter|\n",
      "+----+----+----+---+-----+------+----+-----+---+---------+-------+\n",
      "|   0|   0|   0|  0|    0|     0|   0|    0|  0|        0|      0|\n",
      "+----+----+----+---+-----+------+----+-----+---+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "print(\"Null values in each column:\")\n",
    "null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Summary:\n",
      "+-------+------------------+------------------+------------------+-----------------+--------------------+\n",
      "|summary|              Open|              High|               Low|            Close|              Volume|\n",
      "+-------+------------------+------------------+------------------+-----------------+--------------------+\n",
      "|  count|              1257|              1257|              1257|             1257|                1257|\n",
      "|   mean|213.31521943910974|218.11713001248185|208.20468772244814|213.2794425815652|1.2556862633253779E8|\n",
      "| stddev| 83.44924973788747| 85.22529577503349| 81.38276862266676|83.32151523455335| 8.210821218093796E7|\n",
      "|    min|24.979999542236328| 26.99066734313965|23.367332458496094|24.08133316040039|            29401800|\n",
      "|    max| 475.8999938964844| 488.5400085449219|  457.510009765625|479.8599853515625|           914082000|\n",
      "+-------+------------------+------------------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics for numerical columns\n",
    "print(\"Statistical Summary:\")\n",
    "df.select(\"Open\", \"High\", \"Low\", \"Close\", \"Volume\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Date Range:\n",
      "Start Date: 2020-01-02\n",
      "End Date: 2024-12-30\n",
      "Total Trading Days: 1,257\n"
     ]
    }
   ],
   "source": [
    "# Date range of the dataset\n",
    "date_range = df.select(\n",
    "    F.min(\"Date\").alias(\"Start_Date\"),\n",
    "    F.max(\"Date\").alias(\"End_Date\"),\n",
    "    F.count(\"Date\").alias(\"Trading_Days\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Dataset Date Range:\")\n",
    "print(f\"Start Date: {date_range['Start_Date']}\")\n",
    "print(f\"End Date: {date_range['End_Date']}\")\n",
    "print(f\"Total Trading Days: {date_range['Trading_Days']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Technical Indicators and Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical indicators calculated.\n",
      "+----------+------------------+-------------------+--------------------+------------------+\n",
      "|      Date|             Close|       Daily_Return|        Daily_Change|    Intraday_Range|\n",
      "+----------+------------------+-------------------+--------------------+------------------+\n",
      "|2020-01-02| 28.68400001525879|               null|                null|0.5993328094482422|\n",
      "|2020-01-03|29.534000396728516| 2.9633258297920753|  0.8500003814697266|1.1386661529541016|\n",
      "|2020-01-06|  30.1026668548584| 1.9254637045135075|  0.5686664581298828|0.7706680297851562|\n",
      "|2020-01-07|31.270666122436523|  3.880052465815389|   1.167999267578125|1.2179985046386719|\n",
      "|2020-01-08| 32.80933380126953|  4.920482578812169|  1.5386676788330078| 2.017332077026367|\n",
      "|2020-01-09|32.089332580566406|-2.1945011900097318|  -0.720001220703125|1.7286663055419922|\n",
      "|2020-01-10|31.876667022705078| -0.662729763317422|-0.21266555786132812|0.7493343353271484|\n",
      "|2020-01-13|34.990665435791016|  9.768895885093325|  3.1139984130859375|2.2420005798339844|\n",
      "|2020-01-14|35.861331939697266| 2.4882822120200907|    0.87066650390625|1.5006675720214844|\n",
      "|2020-01-15| 34.56666564941406|-3.6102013513057836| -1.2946662902832031|1.4033317565917969|\n",
      "+----------+------------------+-------------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate daily returns and price changes\n",
    "window_spec = Window.orderBy(\"Date\")\n",
    "\n",
    "df = df.withColumn(\"Prev_Close\", F.lag(\"Close\", 1).over(window_spec)) \\\n",
    "       .withColumn(\"Daily_Return\", ((F.col(\"Close\") - F.col(\"Prev_Close\")) / F.col(\"Prev_Close\")) * 100) \\\n",
    "       .withColumn(\"Daily_Change\", F.col(\"Close\") - F.col(\"Prev_Close\")) \\\n",
    "       .withColumn(\"Intraday_Range\", F.col(\"High\") - F.col(\"Low\")) \\\n",
    "       .withColumn(\"Intraday_Change_Pct\", ((F.col(\"Close\") - F.col(\"Open\")) / F.col(\"Open\")) * 100)\n",
    "\n",
    "print(\"Technical indicators calculated.\")\n",
    "df.select(\"Date\", \"Close\", \"Daily_Return\", \"Daily_Change\", \"Intraday_Range\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving averages calculated.\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+\n",
      "|      Date|             Close|              MA_7|             MA_30|             MA_50|            MA_200|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+\n",
      "|2020-01-02| 28.68400001525879| 28.68400001525879| 28.68400001525879| 28.68400001525879| 28.68400001525879|\n",
      "|2020-01-03|29.534000396728516|29.109000205993652|29.109000205993652|29.109000205993652|29.109000205993652|\n",
      "|2020-01-06|  30.1026668548584|29.440222422281902|29.440222422281902|29.440222422281902|29.440222422281902|\n",
      "|2020-01-07|31.270666122436523|29.897833347320557|29.897833347320557|29.897833347320557|29.897833347320557|\n",
      "|2020-01-08| 32.80933380126953| 30.48013343811035| 30.48013343811035| 30.48013343811035| 30.48013343811035|\n",
      "|2020-01-09|32.089332580566406| 30.74833329518636| 30.74833329518636| 30.74833329518636| 30.74833329518636|\n",
      "|2020-01-10|31.876667022705078|30.909523827689036|30.909523827689036|30.909523827689036|30.909523827689036|\n",
      "|2020-01-13|34.990665435791016| 31.81047603062221|31.419666528701782|31.419666528701782|31.419666528701782|\n",
      "|2020-01-14|35.861331939697266|  32.7143805367606| 31.91318490770128| 31.91318490770128| 31.91318490770128|\n",
      "|2020-01-15| 34.56666564941406|33.352094650268555| 32.17853298187256| 32.17853298187256| 32.17853298187256|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate moving averages\n",
    "# Define window specifications for moving averages\n",
    "window_7 = Window.orderBy(\"Date\").rowsBetween(-6, 0)\n",
    "window_30 = Window.orderBy(\"Date\").rowsBetween(-29, 0)\n",
    "window_50 = Window.orderBy(\"Date\").rowsBetween(-49, 0)\n",
    "window_200 = Window.orderBy(\"Date\").rowsBetween(-199, 0)\n",
    "\n",
    "df = df.withColumn(\"MA_7\", F.avg(\"Close\").over(window_7)) \\\n",
    "       .withColumn(\"MA_30\", F.avg(\"Close\").over(window_30)) \\\n",
    "       .withColumn(\"MA_50\", F.avg(\"Close\").over(window_50)) \\\n",
    "       .withColumn(\"MA_200\", F.avg(\"Close\").over(window_200)) \\\n",
    "       .withColumn(\"Volume_MA_30\", F.avg(\"Volume\").over(window_30))\n",
    "\n",
    "print(\"Moving averages calculated.\")\n",
    "df.select(\"Date\", \"Close\", \"MA_7\", \"MA_30\", \"MA_50\", \"MA_200\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volatility and RSI calculated.\n"
     ]
    }
   ],
   "source": [
    "# Calculate volatility (30-day rolling standard deviation)\n",
    "df = df.withColumn(\"Volatility_30\", F.stddev(\"Daily_Return\").over(window_30))\n",
    "\n",
    "# Calculate RSI (Relative Strength Index)\n",
    "df = df.withColumn(\"Gain\", F.when(F.col(\"Daily_Change\") > 0, F.col(\"Daily_Change\")).otherwise(0)) \\\n",
    "       .withColumn(\"Loss\", F.when(F.col(\"Daily_Change\") < 0, -F.col(\"Daily_Change\")).otherwise(0))\n",
    "\n",
    "window_14 = Window.orderBy(\"Date\").rowsBetween(-13, 0)\n",
    "df = df.withColumn(\"Avg_Gain\", F.avg(\"Gain\").over(window_14)) \\\n",
    "       .withColumn(\"Avg_Loss\", F.avg(\"Loss\").over(window_14)) \\\n",
    "       .withColumn(\"RS\", F.col(\"Avg_Gain\") / F.col(\"Avg_Loss\")) \\\n",
    "       .withColumn(\"RSI\", 100 - (100 / (1 + F.col(\"RS\"))))\n",
    "\n",
    "print(\"Volatility and RSI calculated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESLA STOCK ANALYSIS COMPLETE\n",
      "============================================================\n",
      "\n",
      "DataFrame is ready for further analysis.\n",
      "You can now run SQL queries, create visualizations, or perform additional analysis.\n",
      "\n",
      "To stop the Spark session, run: spark.stop()\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"TESLA STOCK ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nDataFrame is ready for further analysis.\")\n",
    "print(\"You can now run SQL queries, create visualizations, or perform additional analysis.\")\n",
    "print(\"\\nTo stop the Spark session, run: spark.stop()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark(Initialbid)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
